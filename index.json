[{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand the basics of AWS and how to create an AWS Free Tier account. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 09/08/2025 09/08/2025 3 - Research the cloud computing technology of Amazon Web Services. + What is Cloud Computing? + what make AWS Different? + How to Start Your Cloud Journey + AWS Global Infrastructure 09/09/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learning AWS Services: + AWS Budgets + AWS Identity and Access Management (IAM) + AWS Support 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Apply Cost Management Services + Access Management + Technical Support 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what Amazon Web Services (AWS) cloud computing is and grasp the basic foundation:\nCloud Computing is the on-demand delivery of IT resources over the Internet with a pay-as-you-go pricing model. What Makes AWS Different is that it has been the leading cloud infrastructure provider for 15 consecutive years [as of the end of 2025], with a core focus on delivering real customer value in all of its leadership principles. Starting Your Cloud Journey can be done by self-learning through AWS training courses. AWS Global Infrastructure includes Regions → Availability Zones (multiple independent data centers within a Region) → Edge Locations (content delivery closer to users) → Local Zones (resources near major cities) → Wavelength Zones (5G integration) → Outposts (AWS infrastructure deployed in customer data centers). Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and Using AWS Services::\nCost Management – AWS Budgets Access Management – AWS Identity and Access Management (IAM) Technical Support – AWS Support "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Studied AWS Networking Services. Learn the theory and practice of basic EC2. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study the basic EC2 theory: + Instance types + AMI + EBS - remote SSH into EC2. - Study Elastic IP. - Practice: + Create EC2 instance + Connect SSH emsp; 09/14/2025 09/14/2025 https://000004.awsstudygroup.com/vi 3 - Studied AWS Networking Services + Amazon Virtual Private Cloud ( VPC ); + VPC Peering \u0026amp; Transit Gateway + VPN \u0026amp; Direct Connect + Elastic Load Balancing \u0026amp;emsp 09/15/2025 09/17/2025 https://www.youtube.com/watch?v=O9Ac_vGHquM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=25 4 - Studied AWS Networking Services + Amazon Virtual Private Cloud ( VPC ); + VPC Peering \u0026amp; Transit Gateway + VPN \u0026amp; Direct Connect + Elastic Load Balancing \u0026amp;emsp 09/15/2025 09/17/2025 https://000003.awsstudygroup.com/vi 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 09/16/2025 09/17/2025 https://000003.awsstudygroup.com/vi/1-introduce 6 - Group meeting about project ideas and writing worklog 09/18/2025 09/18/2025 Week 2 Achievements: Study the basic EC2 theory::\nInstance types: different virtual hardware configurations provided by AWS to run EC2 instances, each optimized for specific use cases. AMI: a template to launch EC2 instances, including the operating system, software, and volume configuration. Ways to remote SSH into EC2: using an SSH Client with a key pair, EC2 Instance Connect via browser, Session Manager without opening port 22, and PuTTY on Windows with a .ppk file. Elastic IP: a static, public IPv4 address allocated to an AWS account and attachable/detachable to EC2 instances, ensuring a fixed public IP for stable external access. Successfully created and connected to EC2 via SSH.\nStudy AWS Networking Services:\nAmazon Virtual Private Cloud (VPC): a customizable virtual network in AWS Cloud that allows you to create an isolated and secure networking environment. VPC Peering \u0026amp; Transit Gateway: VPC Peering directly connects two VPCs via private IPs (simple but no transitive routing), while Transit Gateway acts as a central hub to connect multiple VPCs and on-premises networks, supporting transitive routing, scalability, and centralized management. VPN \u0026amp; Direct Connect: VPN connects on-premises networks to AWS over the Internet via encrypted tunnels (easy to deploy but Internet-dependent), while Direct Connect provides a dedicated connection with higher stability, bandwidth, and lower latency, though costlier and slower to set up. Elastic Load Balancing (ELB): automatically distributes traffic across multiple resources to improve availability, scalability, and fault tolerance, with ALB (HTTP/HTTPS), NLB (high-performance TCP/UDP), and GWLB (virtual appliances). "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn about Compute VM services on AWS. Practice with Compute VM services on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Compute VM services on AWS. + Amazon Elastic Compute Cloud (EC2) + Amazon Lightsail + Amazon EFS / FSX + AWS Application Migration Service (MGN)regulations 09/15/2025 09/16/2025 https://www.youtube.com/watch?v=-t5h4N6vfBs\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=72 3 - Learn about Compute VM services on AWS. + Amazon Elastic Compute Cloud (EC2) + Amazon Lightsail + Amazon EFS / FSX + AWS Application Migration Service (MGN)regulations 09/15/2025 09/16/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 4 - Practice: + Deploy AWS Backup to the System + Using AWS File Storage Gateway + Create SH3 Bucket + Using Autoscaling Group 09/17/2025 09/19/2025 https://000013.awsstudygroup.com/vi 5 - Practice: + Deploy AWS Backup to the System + Using AWS File Storage Gateway + Create SH3 Bucket + Using Autoscaling Group 09/17/2025 09/19/2025 https://000024.awsstudygroup.com 6 - Practice: + Deploy AWS Backup to the System + Using AWS File Storage Gateway + Create SH3 Bucket + Using Autoscaling Group 09/17/2025 09/19/2025 https://000024.awsstudygroup.com Week 3 Achievements: Understood AWS Compute VM services:\nAmazon Elastic Compute Cloud (EC2): Amazon EC2 is similar to a virtual or traditional physical server. It provides fast provisioning, strong resource scalability, and flexibility. Amazon Lightsail: a low-cost compute service (pricing starts at $3.5/month). Each Lightsail instance includes a data transfer allocation (cheaper than EC2), suitable for light workloads, dev/test environments, and applications that do not require high CPU usage continuously for more than 2 hours per day. Amazon EFS / FSx: EFS (Elastic File System): allows creation of NFSv4 network volumes that can be mounted to multiple EC2 instances simultaneously, with storage scaling up to petabytes. EFS only supports Linux and charges based on used storage. It can be mounted to on-premises environments via Direct Connect or VPN. FSx: allows creation of NTFS volumes mountable to multiple EC2 instances using the SMB protocol, supports Windows and Linux, and charges based on used storage. AWS Application Migration Service: used to migrate and replicate servers for building Disaster Recovery sites, continuously copying source physical or virtual servers to EC2 instances in AWS (asynchronously or synchronously). Successfully deployed AWS Backup, File Storage Gateway, and Auto Scaling Group..\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Discuss and brainstorm project ideas with the group. Learn about storage services on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about storage services on AWS. + Amazon Simple Storage Service - S3 + Amazon Storage Gateway + Snow Family + Disaster Recovery on AWS emsp; + AWS Backup 09/22/2025 09/23/2025 https://www.youtube.com/watch?v=hsCfP0IxoaM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=103 3 - Learn about storage services on AWS. + Amazon Simple Storage Service - S3 + Amazon Storage Gateway + Snow Family + Disaster Recovery on AWS emsp; + AWS Backup 09/22/2025 09/23/2025 https://www.youtube.com/watch?v=hsCfP0IxoaM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=103 4 - Perform Lab on AWS storage services. - Practice: + VM Import/Export + Deploy File Storage Gateway 09/24/2025 09/25/2025 https://000014.awsstudygroup.com/vi https://000024.awsstudygroup.com/vi 5 - Perform Lab on AWS storage services. - Practice: + VM Import/Export + Deploy File Storage Gateway 09/24/2025 09/25/2025 https://000014.awsstudygroup.com/vi https://000024.awsstudygroup.com/vi 6 - Group meeting about project ideas and writing worklog 09/26/2025 09/26/2025 Week 4 Achievements: Learned about AWS storage services:\nAmazon Simple Storage Service - S3 Amazon Storage Gateway Snow Familys Disaster Recovery on AWS Successfully imported/exported VM and deployed File Storage Gateway.\nCompleted writing the worklog and finalized the project idea.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn about security services on AWS. Do the lab. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about security services on AWS. + Shared Responsibility Model - AWS Identity and Access Management + Amazon Cognito + AWS Organization \u0026amp; AWS Identity Center ( SSO ) + AWS KMS 09/29/2025 09/30/2025 https://www.youtube.com/watch?v=tsobAlSg19g\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=150 3 - Learn about security services on AWS. + Shared Responsibility Model - AWS Identity and Access Management + Amazon Cognito + AWS Organization \u0026amp; AWS Identity Center ( SSO ) + AWS KMS 09/29/2025 09/30/2025 https://www.youtube.com/watch?v=tsobAlSg19g\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=150 4 Practice: + Compete Lab 18 about AWS Security Hub 10/01/2025 10/01/2025 https://000018.awsstudygroup.com 5 Practice: + Compete lab 22 \u0026amp; 27 about Optimizing EC2 Costs with Lambda và Manage Resources Using Tags and Resource Groups 10/02/2025 10/03/2025 https://000027.awsstudygroup.com https://000022.awsstudygroup.com 6 Practice: + Compete lab 22 \u0026amp; 27 about Optimizing EC2 Costs with Lambda và Manage Resources Using Tags and Resource Groups 10/02/2025 10/03/2025 https://000027.awsstudygroup.com https://000022.awsstudygroup.com Week 5 Achievements: Understand AWS security services:\nShared Responsibility Model: The AWS security model defines the division of responsibilities between AWS and customers in protecting systems and data on the cloud platform. AWS Identity and Access Management (IAM): Manages user identities, roles, and permissions to securely control access to AWS resources. Amazon Cognito: Provides authentication, authorization, and user management for web and mobile applications. AWS Organization \u0026amp; AWS Identity Center (SSO): Enables centralized management of multiple AWS accounts, unified access control, and single sign-on for users across the organization. AWS KMS: Manages encryption keys used to protect data, allowing secure creation, storage, and control of keys. Understand the structure of AWS Security Hub.\nSuccessfully completed Lab 22 \u0026amp; 27.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Continue working on the labs from Module 5. Understand basic AWS services and how to use both the AWS Management Console and AWS CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Practice: + Do lab 28 about MANAGE ACCESS TO EC2 SERVICES WITH RESOURCE TAGS THROUGH IAM SERVICES 10/06/2025 10/06/2025 https://000028.awsstudygroup.com 3 Practice: + Do lab 30 \u0026amp; 33 about LIMITATION OF USER RIGHTS WITH IAM PERMISSION BOUNDARY and Encrypt at rest with AWS KMS 10/07/2025 10/07/2025 https://000030.awsstudygroup.com https://000033.awsstudygroup.com 4 Practice: + Do lab 44 \u0026amp; 48 about IAM Role \u0026amp; Condition and Granting authorization for an application to access AWS services with an IAM role. 10/08/2025 10/08/2025 https://000044.awsstudygroup.com https://000048.awsstudygroup.com 5 - Learn about Database Services on AWS: + Database Concepts + Amazon RDS + Amazon Aurora + Amazon RedShift + Amazon ElastiCache 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=OOD2RwWuLRw\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=217 6 - Group meeting to draw the system diagram 10/10/2025 10/10/2025 Week 6 Achievements: Learned how to manage access to EC2 services using Resource Tags with AWS IAM. Successfully completed Lab 30 \u0026amp; 33, understanding IAM Permission Boundaries (limiting user permissions) and data encryption at rest using AWS KMS. Gained deeper understanding of IAM Roles \u0026amp; Conditions, and how to grant applications access to AWS services using IAM Roles. Acquired additional knowledge about AWS Database Services: Database Concepts Amazon RDS: A managed relational database service that supports multiple database engines such as MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server. It automates tasks like backup, patching, and scaling. Amazon Aurora: A high-performance, fully managed relational database compatible with MySQL and PostgreSQL, designed for scalability and high availability. Amazon RedShift: A fully managed data warehouse service optimized for large-scale data analysis (OLAP) and big data analytics. Amazon ElastiCache: A managed in-memory caching service supporting Redis and Memcached, improving application performance by reducing database load and query latency. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Complete the labs from module 6. Understand basic AWS services and how to use both the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Lab practice for module 6: + Complete Lab 5 on Amazon Relational Database Service (Amazon RDS) + Complete Lab 43 on schema conversion and database migration 10/13/2025 10/14/2025 https://000005.awsstudygroup.com https://000043.awsstudygroup.com 3 Lab practice for module 6: + Complete Lab 5 on Amazon Relational Database Service (Amazon RDS) + Complete Lab 43 on schema conversion and database migration 10/13/2025 10/14/2025 https://000005.awsstudygroup.com https://000043.awsstudygroup.com 4 - Research and work on the group project 10/15/2025 10/17/2025 5 - Research and work on the group project 10/15/2025 10/17/2025 6 - Research and work on the group project 10/15/2025 10/17/2025 Week 7 Achievements: Successfully completed Lab for module 6\nMaking good progress on researching and working on the group project\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Review lessons to prepare for the midterm exam Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 3 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 4 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 5 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 6 - Review knowledge to prepare for the midterm exam 10/20/2025 10/24/2025 Week 8 Achievements: Still in the review process. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Review lessons and take the midterm exam. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review knowledge to prepare for the midterm exam 10/27/2025 10/29/2025 3 - Review knowledge to prepare for the midterm exam 10/27/2025 10/29/2025 4 - Review knowledge to prepare for the midterm exam 10/27/2025 10/29/2025 5 - Take the midterm exam 10/30/2025 10/30/2025 6 - Research and work on the group project 10/31/2025 10/31/2025 Week 9 Achievements: Finished reviewing and completed the midterm exam. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learned the basics of EC2, successfully deployed and connected instances, and understood AWS networking services.\nWeek 3: Understood AWS Compute VM services and successfully deployed AWS Backup, File Storage Gateway, S3 bucket, and Autoscaling Group.\nWeek 4: Learned AWS storage services, practiced VM Import/Export and deployed File Storage Gateway, completed the worklog, and finalized the project idea.\nWeek 5: Understood AWS security services and completed labs on EC2 cost optimization and resource management.\nWeek 6: Completed IAM labs, gained basic knowledge of AWS KMS, and explored AWS database services.\nWeek 7: Completed module 6 labs (RDS and database migration) and made progress on researching and implementing the group project.\nWeek 8: In the process of reviewing for the midterm exam.\nWeek 9: Reviewed and completed the midterm exam.\nWeek 10: Continued progress in researching and implementing the group project.\nWeek 11: Continued progress in researching and implementing the group project.\nWeek 12: Continued progress in researching and implementing the group project.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Building a Speech Intelligence System with AssemblyAI\u0026rsquo;s Speech-to-Text Model from AWS Marketplace Author: Shun Mao and Zackary Klebanoff\nPublished: April 16, 2025\nSource: AWS Marketplace Blog\nIntroduction Speech intelligence technology and Speech-to-Text (STT) have become essential as organizations collect thousands of hours of call recordings, meetings, and customer interactions each day. However, raw audio data alone cannot provide actionable insights — organizations need the capability to leverage intelligence to extract value from speech data at scale.\nSpeech intelligence combines speech recognition technology, natural language processing (NLP), and machine learning (ML) to convert audio data into actionable insights. Modern STT models are capable of transcribing conversations with high accuracy and work in tandem with other auxiliary tools to analyze sentiment, detect main topics, and create automatic summaries to provide deeper insights.\nSpeech intelligence and STT technologies are widely applied in various industries, such as call analytics, conversational intelligence, medical transcription, customer service, video content optimization, legal compliance, sales training and analysis, and more. With the rise of generative AI and increasingly advanced models, the demand for efficient STT models continues to grow in these applications.\nAbout AssemblyAI AssemblyAI, an Independent Software Vendor (ISV) on AWS Marketplace, is a research-driven organization focused on advancing and democratizing speech AI technology globally. Founded in 2017, the company has built a team of interdisciplinary researchers, scientists, and engineers working towards developing state-of-the-art speech AI models, unlocking new possibilities for audio-based applications.\nAssemblyAI\u0026rsquo;s technology serves thousands of customers and hundreds of thousands of developers worldwide through a simple, developer-friendly API. AssemblyAI provides comprehensive speech AI capabilities, including:\nCore Speech-to-Text\nSpeaker Detection\nAutomatic Language Detection\nSentiment Analysis\nChapter Detection\nPII (Personally Identifiable Information) Redaction\nAssemblyAI\u0026rsquo;s Universal-2 model demonstrates the company\u0026rsquo;s commitment to pushing the limits of speech AI technology. This model achieves high accuracy by addressing key challenges in speech recognition, improving the recognition of proper nouns, text formatting, capitalization, and accurate timestamp creation. AssemblyAI takes a research-based approach to building robust and easily integrable speech AI models.\nThis article will guide you on how to get started with AssemblyAI\u0026rsquo;s API on AWS Marketplace, while also building an initial Proof of Concept (POC) by making API calls in just a few simple steps.\nSolution Overview AssemblyAI\u0026rsquo;s Speech-to-Text service processes audio through a two-stage pipeline. In the first stage, the system uses the Universal-2 ASR (Automatic Speech Recognition) model — a Conformer RNN-T model with 600 million parameters, trained on 12.5 million hours of multilingual audio data. This model can transcribe speech to text while effectively handling complex situations, such as multiple speakers, regional accents, and background noise. In the second stage, the system applies neural models to format the text, adding punctuation, capitalization, and text normalization to produce a clean, readable, and professional transcript.\nIn addition to core transcription, users can activate additional AI models that run in parallel with the main ASR process. These models include:\nSpeaker Identification – determining who is speaking in a conversation.\nSentiment Analysis – understanding the tone and sentiment in speech.\nTopic Detection – automatically classifying the conversation\u0026rsquo;s content.\nContent Summarization – extracting key points and summaries.\nPII Redaction – ensuring compliance with privacy requirements.\nAll these models seamlessly integrate through the same API interface, making it easy for developers to incorporate and expand speech analytics capabilities.\nFigure 1: High-level Architecture Diagram for AssemblyAI\u0026rsquo;s Transcription API\nPrerequisites Before you begin, ensure you have the following:\nAn Amazon Web Services (AWS) account with access to Amazon Simple Storage Service (Amazon S3).\nAssemblyAI\u0026rsquo;s API service is available for purchase on AWS Marketplace. You can also sign up for a trial account directly on AssemblyAI’s website.\nOnce you\u0026rsquo;ve created your AssemblyAI account, store your API key securely for use in the next steps.\nNext, run the following Python code to prepare for the implementation scenarios in the solution walkthrough:\n!pip install assemblyai import assemblyai as aai aai.settings.api_key = \u0026quot;xxxxxxxx\u0026quot; # Your AssemblyAI API key Solution Walkthrough In this section, we will explore five use cases where AssemblyAI\u0026rsquo;s API can deliver significant value. Each case is accompanied by a code snippet, enabling readers to experiment within their own environment.\nTranscribing Audio from a Local File\nTranscribing an Audio File from Amazon S3\nSpeaker Diarization\nAutomatic Language Detection\nPII Redaction\nTranscribing Audio from Amazon S3 In many organizations, audio data is stored in cloud storage services like Amazon S3. To transcribe an audio file from an S3 bucket, AssemblyAI requires temporary access to that file.\nTo grant this access, you need to create a pre-signed URL, which is a special link that provides access to the file for a limited time.\nFor more details on how to create a pre-signed URL, refer to the AWS documentation: “Sharing Objects with Pre-signed URLs.”\nExecute the following Python code to perform transcription:\nimport requests import time p_url = \u0026quot;S3 pre-signed url\u0026quot; assembly_key = \u0026quot;xxxxxxxx\u0026quot; # Your AssemblyAI API key # Use the API key for authentication headers = {\u0026quot;authorization\u0026quot;: assembly_key, \u0026quot;content-type\u0026quot;: \u0026quot;application/json\u0026quot;} # AssemblyAI's transcription API endpoint upload_endpoint = \u0026quot;https://api.assemblyai.com/v2/transcript\u0026quot; # Use the pre-signed URL for the audio file in the POST request json = {\u0026quot;audio_url\u0026quot;: p_url} # POST request to queue the audio file for transcription post_response = requests.post(upload_endpoint, json=json, headers=headers) # Get the processing endpoint get_endpoint = upload_endpoint + \u0026quot;/\u0026quot; + post_response.json()[\u0026quot;id\u0026quot;] # GET request to check the transcription status get_response = requests.get(get_endpoint, headers=headers) # If transcription is not yet complete, wait until it's finished while get_response.json()[\u0026quot;status\u0026quot;] != \u0026quot;completed\u0026quot;: get_response = requests.get(get_endpoint, headers=headers) time.sleep(5) # When transcription is complete, print the result print(get_response.json()[\u0026quot;text\u0026quot;]) Transcribing Audio from a Local File This is a simple setup where audio files are stored in the local directory where the code is executed.\nAssemblyAI\u0026rsquo;s API supports most popular audio and video formats like mp3, m4a, m4p, wav, or wma.\nFor optimal recognition quality, it\u0026rsquo;s recommended to use the raw format of the audio file rather than converting it to another format.\nFor more details about supported audio file formats, refer to AssemblyAI’s blog.\nDownload a publicly available audio file from AssemblyAI\u0026rsquo;s website, and save it to your local directory.\nExecute the following code to transcribe the file:\n# Transcribe a local audio file transcriber = aai.Transcriber() transcript = transcriber.transcribe(\u0026quot;./Audios/ford_clip_trimmed.mp3\u0026quot;) print(transcript.text) Speaker Diarization Speaker diarization is an important part of audio processing, as it addresses the challenge of identifying who speaks and when in a recording. This capability is essential for many tasks, such as improving clarity and structure, supporting advanced analytics, and enabling content personalization.\nExecute the following code to perform transcription:\nconfig = aai.TranscriptionConfig(speaker_labels=True) transcriber = aai.Transcriber(config=config) FILE_URL = \u0026quot;https://github.com/AssemblyAI-Examples/audio-examples/raw/main/20230607_me_canadian_wildfires.mp3\u0026quot; transcript = transcriber.transcribe(FILE_URL) # Extract all the speech utterances from the response utterances = transcript.utterances # For each utterance, print the speaker and their speech for utterance in utterances: speaker = utterance.speaker text = utterance.text print(f\u0026quot;Speaker {speaker}: {text}\u0026quot;) Automatic Language Detection Automatic language detection is another key feature in audio analysis, as it helps the system process and understand spoken content more accurately and effectively.\nThis feature enhances the user experience in multilingual applications and allows for customization based on specific languages.\nExecute the following code to perform transcription:\nconfig = aai.TranscriptionConfig(language_detection=True) transcriber = aai.Transcriber(config=config) FILE_URL = \u0026quot;https://assembly.ai/news.mp4\u0026quot; transcript = transcriber.transcribe(FILE_URL) print(transcript.json_response['language_code']) PII Redaction Security is a top priority at AWS and AssemblyAI.\nThe PII redaction feature provided by AssemblyAI helps maintain privacy and security for sensitive information, allowing customers to build secure and compliant applications without legal or regulatory risks.\nUsers can control which types of sensitive data (e.g., credit card numbers, email addresses, phone numbers) to redact through configuration, as demonstrated in the following code snippet:\nconfig = aai.TranscriptionConfig() config.set_redact_pii( policies=[ aai.PIIRedactionPolicy.credit_card_number, aai.PIIRedactionPolicy.email_address, aai.PIIRedactionPolicy.location, aai.PIIRedactionPolicy.person_name, aai.PIIRedactionPolicy.phone_number, ], substitution=aai.PIISubstitutionPolicy.hash, ) transcriber = aai.Transcriber(config=config) FILE_URL = \u0026quot;https://example.org/audio.mp3\u0026quot; transcript = transcriber.transcribe(FILE_URL) print(transcript.text) Conclusion AssemblyAI is committed to building a high-quality API platform for developers to convert and understand speech data using AI, enabling the creation of innovative products and services.\nAssemblyAI\u0026rsquo;s speech-to-text models address critical challenges in the transcription process, and their latest model — Universal-2 — focuses on solving \u0026ldquo;edge-case\u0026rdquo; issues that impact real-world speech AI, such as improving accuracy for numbers, special characters, and rare words.\nLearn more about the improvements in Universal-2: [Read AssemblyAI\u0026rsquo;s Blog] See how AssemblyAI compares to competitors: [View Performance Comparison Chart] Explore the research behind Universal-2: [Learn More About Research]\nYou can start using AssemblyAI\u0026rsquo;s API by visiting the product page on AWS Marketplace or by creating an account directly on AssemblyAI\u0026rsquo;s website.\nAbout the Authors Shun Mao\nShun Mao is a Senior Partner Solution Architect in the Independent Software Vendor (ISV) partner group, specializing in Artificial Intelligence and Machine Learning (AI/ML) at Amazon Web Services (AWS). He has many years of experience in data science, analytics, AI, and cloud computing across various industries, including oil and gas and pharmaceuticals. At AWS, he supports strategic AI/ML partners in developing creative products and solutions that deliver business value to customers. Outside of work, Shun enjoys fishing, traveling, and playing table tennis. Zackary Klebanoff\nZackary Klebanoff is a Senior Solution Architect at AssemblyAI. He has several years of experience helping customers leverage speech AI technology to build exceptional products and applications. Throughout his career, he has worked at startups, helping them grow by bridging the gap between technology and practical applications. Outside of work, Zackary enjoys playing basketball, attending concerts, and supporting the Philadelphia Eagles football team. "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Tapestry Makes Enterprise Knowledge More Accessible with Generative AI on AWS Authors: Nishant Singh, Aditya Pendyala, Aravind Narasimhan, and Karthigeyan Ramakrishnan\nPublished: February 26, 2025\nCategory: Amazon Aurora, Amazon Bedrock, Amazon Bedrock Knowledge Bases, Amazon CloudFront, Amazon S3, AWS IAM, AWS Lambda, Retail\nIntroduction Generative AI has the potential to change the way large companies operate. For businesses, information is often stored in silos across different teams. By combining large language models (LLMs) with powerful knowledge bases, organizations can create intelligent systems that not only store information but also understand it—making enterprise knowledge more accessible and usable than ever before.\nTapestry, a technology company that owns iconic brands like Coach and Kate Spade, is leading the way in applying generative AI to transform their enterprise knowledge management model. By leveraging AWS, the company has developed an intelligent information management system that centralizes data access across business units, helping employees make faster and better decisions.\nTransforming Enterprise Knowledge Management with AI on AWS Tapestry has nurtured a technology innovation culture in the luxury retail industry, demonstrated by their experimentation and deployment of new solutions. This commitment is evident in their approach to AI: calculated, secure, and aimed at creating business value.\n“We wanted to build an internal generative AI solution before experimenting in the public space,” shared Aravind Narasimhan, Vice President of Applied Technology at Tapestry. “This gives us an opportunity to experiment with the technology, learn, and educate partners within the company.” Like many large enterprises, Tapestry faced challenges with knowledge management. Information was fragmented across departments such as IT, HR, legal, and others, making it difficult for employees to quickly find answers. Standard operating procedures, policies, and internal knowledge were also stored in different systems and formats. Tapestry saw an opportunity to use generative AI to address this issue.\nAmazon Bedrock—providing high-performance foundational models—along with Amazon Bedrock Knowledge Bases, which allows these foundational models and agents to access contextual information from the company’s private data sources—has provided the ideal foundation for building an enterprise-scale solution.\n“Amazon Bedrock Knowledge Bases is a plug-and-play solution with minimal coding requirements,” said Karthigeyan Ramakrishnan, Director of Applications at Tapestry. “You can choose embedding models, vector stores, and LLMs. You can easily configure chunking, tokens, and other parts. That’s why we love it.” Building an Enterprise Platform to Share Information Smoothly The development process lasted 4 months, starting with a proof of concept in the IT department. The knowledge management system uses multiple AWS services. At its core, Amazon Bedrock Knowledge Bases is used to manage and organize the document repository, while Claude 3 Haiku serves as the large language model to handle natural language queries.\nTo create text embeddings that reflect semantic meaning, Tapestry uses the Titan foundation models in Amazon Bedrock, offering a variety of options for image, multimodal, and text models.\nAdditionally, Amazon Aurora for PostgreSQL—with high performance and exceptional availability at a global scale—was used as a vector store and index to support fast and efficient semantic search across millions of document segments.\nTapestry’s development team designed the knowledge management system with separate public and private knowledge bases, each configured independently to meet the needs of each department. The public knowledge base is accessible to everyone in the company, containing general operational information, while private knowledge bases are restricted through role-based access control (RBAC) for sensitive department-specific data.\nTo implement this separation, they used two methods:\nRoles in AWS Identity and Access Management (IAM) to manage identity security and access to AWS services and resources.\nIntegration of the solution with Tapestry’s single sign-on (SSO) system.\nThe serverless architecture of the solution provides a seamless user experience while ensuring global scalability. Employees can access the knowledge management system from virtually anywhere in the world through the web-based chatbot interface, with content distributed securely by Amazon CloudFront.\nWhen users submit queries, AWS Lambda—running event-driven code and automatically managing computing resources—triggers functions to process the requests, perform necessary security checks, and route queries to the appropriate knowledge base.\nSource documents and metadata are stored in Amazon S3—an object storage service that allows large amounts of data to be retrieved from anywhere.\nA standout feature of the knowledge management system is its ability to keep content fresh and up-to-date. The Tapestry team has implemented automated processes to scan and update the knowledge bases, ensuring that any changes to the source documents are reflected in the system.\nThey developed an automated pipeline to monitor the repositories for changes, process new or edited documents using appropriate chunking strategies for each type of content, create new embeddings using Amazon\u0026rsquo;s Titan foundation models, and automatically index those new embeddings in the Aurora PostgreSQL vector store to keep the knowledge base updated.\nCreating Business Value Through Enhanced Knowledge Management Tapestry is expanding the system’s capabilities to make it an even more powerful solution. Plans include deploying an image search tool to extract and query information from visual content, as well as integrating the solution into other enterprise systems to generate near-real-time reports and data analytics.\n“We’re working to make the system smarter and more interactive with user feedback,” Ramakrishnan shared. “Currently, we have text search, and we’re looking into how we can add voice capabilities. We’re also working on bringing structured data into the system, where users can ask questions about sales, inventory, store traffic, and orders.” Thanks to generative AI on AWS, Tapestry is breaking down information silos, accelerating decision-making, and preserving valuable organizational knowledge that would otherwise be at risk of being lost. The solution has significantly reduced the time employees spend searching for information and waiting for expertise from within the company. At the same time, these capabilities enable employees to focus on more strategic and innovative work.\n“Our departments benefit from generative AI on AWS because they have to answer fewer routine questions, and from a user perspective, they get what they want faster,” Narasimhan said. “We’ve become more agile and can act faster.” About the Authors Nishant Singh\nNishant is a Senior Customer Solutions Manager (CSM) at AWS, where he focuses his expertise on the retail and consumer packaged goods (CPG) industries. His mission is to help customers design and implement value-based solutions that deliver measurable business outcomes through AWS technologies. With a \u0026quot;customer-first\u0026quot; mindset, he focuses on transforming business challenges into opportunities to drive growth, innovation, and operational efficiency by leveraging the full capabilities of the AWS platform. Aditya Pendyala\nAditya is a Principal Solutions Architect at AWS, based in New York (NYC). He has extensive experience in designing cloud-based applications. Currently, he collaborates with large enterprises to help them build scalable, flexible, and resilient cloud architectures, while providing comprehensive cloud solution consulting. Aditya holds a Master’s in Computer Science from Shippensburg University and believes in the saying, \u0026quot;When you stop learning, you stop growing.\u0026quot; Aravind Narasimhan\nAravind Narasimhan is a seasoned senior leader with a strategic vision for IT, system architecture, technology evaluation, and feasible planning in high-performance environments. Throughout his career, he has led technical implementations for Omni, Cloud, and ERP transformation projects, as well as built and developed Robotic Process Automation (RPA) initiatives. Additionally, Aravind leads the AI Center of Excellence (COE) at Tapestry, where he drives innovation and enhances efficiency in the AI space. Prior to joining Tapestry, he led several initiatives in areas like planning, sales, POS systems, and BI solutions in retail. Aravind holds a degree in Engineering from Bangalore University and an MBA in Finance from Rutgers University. Karthigeyan Ramakrishnan\nKarthigeyan Ramakrishnan is an experienced IT professional with over 20 years in leadership, architecture, consulting, and implementing innovative IT solutions. He has worked in the retail IT industry, covering various functional areas such as warehouse management, planning, procurement, customer service, automation, and next-generation artificial intelligence (Gen AI). Karthigeyan has global consulting experience across markets in Europe, India, South America, and the US. At Tapestry, he heads Planning and Automation. He has a particular passion for Gen AI and is always looking for ways to harness this technology not just to solve problems, but to explore untapped opportunities. Karthigeyan holds a degree in Chemical Engineering from Anna University, Chennai, and enjoys applying reaction design principles to technology solutions. "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"The Agentic Store: How AI Coordination Will Revolutionize Physical Retail Author: Justin Swagler\nPublished: April 16, 2025\nCategory: Amazon Bedrock, Amazon Bedrock Agents, Amazon Elastic Kubernetes Service, Amazon SageMaker, Data Analytics, Generative AI, Industries, Internet of Things (IoT), Retail\nPosted on the AWS Industries Blog Physical retail — which accounts for more than 80% of total retail sales in the United States — is entering a modern-day renaissance. Retailers are modernizing every aspect, from customer experience to internal operations, by harnessing the power of technology.\nHowever, much of these technology investments are creating “technical silos” — where systems operate separately and don’t communicate with each other, resulting in fragmented customer experiences and poor operational performance.\nThis article explores the vision of the \u0026ldquo;Agentic Store\u0026rdquo; — a physical store where every system, device, and person is coordinated intelligently, in real-time, by artificial intelligence (AI), delivering unprecedented efficiency and experience for both businesses and customers.\nChallenge: Technological Fragmentation in Retail Most retailers today face a core barrier that directly affects their competitiveness and growth: a fragmented technology ecosystem.\nAccording to a survey, 84% of retail leaders report struggling with data being siloed across different systems, while only 37% can actually connect and unify online and in-store operations.\nAs a result, even a minor operational failure can trigger a chain of problems. For example, when a store device — like a cooler or vending machine — malfunctions, employees must manually interact with multiple systems: shutting off the device, adjusting the digital menu, reporting the error, updating inventory, notifying staff, opening a maintenance request, etc.\nThese fragmented processes not only waste time but also cause significant financial losses. An IDC study shows that, on average, each hour a POS (Point of Sale) system is down can cost a retail chain more than $5 million.\nAdditionally, \u0026ldquo;technical debt\u0026rdquo; from patchwork solutions — custom systems, asynchronous integrations, and manual updates — is slowing down innovation across the entire organization.\nMeanwhile, new technology waves like IoT, computer vision, contactless payments, smart shelves, and environmental sensors continue to roll out but lack unified connectivity.\nThe consequence is that stores cannot respond to events in real-time, making forecasting, maintenance, or workforce optimization very limited.\nVision: AI-Driven Retail Operations The \u0026ldquo;Agentic Store\u0026rdquo; transforms the current fragmented retail environment into a seamlessly orchestrated ecosystem. Powered by AWS services, it creates a unified platform where AI agents continuously monitor, predict, and coordinate responses across all store systems.\nImagine stepping into a fast-casual restaurant during the busy morning rush. The soda dispenser’s sensor detects an impending malfunction — previously, this situation would trigger a series of manual tasks across various systems, wasting staff time and potentially disrupting customer service. But in the \u0026ldquo;Agentic Store\u0026rdquo;, AI orchestration turns the crisis into a smooth solution. In just a few seconds, the store’s AI agents begin to act: the digital menu board automatically adjusts to remove the affected item, the mobile ordering app updates in real-time to avoid customer frustration, staff receive prioritized task notifications on their devices, while the kitchen system rebalances production schedules. An automatic maintenance ticket is generated, and the smart scheduling system adjusts employee breaks around the anticipated repair time. What used to take hours of manual management now happens in seconds, allowing managers to focus on their team and customers instead of juggling disconnected systems.\nThis coordination capability isn’t limited to restaurants. In a modern supermarket, a computer vision system monitors the organic produce section, where AI agents orchestrate a \u0026ldquo;dance\u0026rdquo; of inventory management. When stock levels drop below the optimal threshold, the system automatically triggers a restocking task, adjusts electronic shelf labels for expiring items, and alerts loyal customers about flash discounts — all without any manual intervention.\nThis transformation extends to convenience stores and gas stations, where personalization meets operational efficiency. When a regular customer pulls into the gas station in the morning, the store system recognizes them and begins orchestrating the experience. The digital screen displays her favorite sandwich, ready, and her favorite coffee is freshly brewed. Inside, the kitchen has already queued her order, while smart screens along her usual shopping path highlight her favorite products. A simple fuel transaction now becomes a personalized shopping journey, seamlessly orchestrated by the connected store technologies.\nFor retailers operating thousands of locations globally, the impact is profound. Traditional stores struggle with disconnected technologies, unsynchronized edge devices, and slow issue resolution. The \u0026ldquo;Agentic Store\u0026rdquo; approach improves incident response times. A global fuel retailer, after integrating its data systems more effectively on AWS, improved its efficiency by 30% in addressing issues like payment failures or device malfunctions before they affected customers. By connecting IoT devices, computer vision systems, customer data, and inventory systems through smart orchestration, stores can operate more efficiently and deliver a superior customer experience.\nThis is the essence of the \u0026ldquo;Agentic Store\u0026rdquo; — a unified ecosystem where AI orchestration turns traditional fragmented operations into a smooth, coordinated experience. It’s not just automation; it’s intelligent coordination, understanding the complex relationships between inventory, workforce, customer preferences, and operational efficiency. The result is a retail environment that is faster, more efficient, capable of delivering exceptional customer experiences, while reducing operational costs and optimizing asset utilization.\nTechnology works in harmony to address issues before they impact consumers or put pressure on staff, creating an environment where both employees and customers benefit from smoother processes and more personalized interactions. This is the future of physical retail — where intelligent orchestration removes the bottlenecks that have long existed in store operations, ushering in a new era of excellence in retail.\nStrategic Roadmap for the Future of Stores The journey to becoming an \u0026ldquo;Agentic Store\u0026rdquo; requires a strategic approach, with clear, incremental steps balancing immediate value creation with long-term transformation. At AWS, we’ve identified a specific roadmap, moving from foundational infrastructure to full-scale reinvention, while maintaining operational stability.\nThis journey starts with building the core platform, focusing on connecting fragmented systems and addressing \u0026ldquo;technical debt.\u0026rdquo; Retailers must first establish a unified data foundation by migrating legacy POS systems to the cloud and connecting critical operational systems. This includes integrating inventory management, HR systems, IoT devices, and customer data platforms. The goal is to create a single unified view of store operations through a database built on AWS and optimized at the edge with Amazon EKS Hybrid Nodes.\nOnce systems are connected, retailers can begin introducing AI through Amazon SageMaker to support predictive analytics and process automation. This phase focuses on enhancing the customer experience through data-driven decision-making. Key capabilities include: real-time inventory optimization with computer vision and IoT sensors, intelligent workforce management with AI-driven task assignments, predictive maintenance for store equipment, automated pricing and promotion management, and frictionless payment capabilities.\nThe final phase is when the traditional store is fully transformed into an autonomously orchestrated environment, where Amazon Bedrock Agents automatically coordinate responses across all store systems. This is when the real differentiation occurs — with store operations automated, the customer experience is personalized through unified data, robots and advanced automation handle repetitive tasks, immersive technology enhances the experience for both staff and customers, and cloud-driven innovation enables rapid deployment of new features.\nThroughout this journey, the key is that each new capability must be built on existing foundations rather than creating new \u0026ldquo;tech silos.\u0026rdquo; Successful transformation requires a focus on training and change management, helping store staff and managers understand how to collaborate effectively with the AI system. Retailers should start with high-impact use cases and clear ROI, as early successes will drive momentum and support for broader transformation.\nThe technology architecture must be built on native cloud services, enabling both edge processing for real-time operations and cloud-based deep data analytics for additional insights. At the same time, comprehensive security measures must be implemented from the start to protect customer data and operational data.\nAs retailers progress further along this journey, they will create stores that are faster, more efficient, and deliver superior customer experiences. The \u0026ldquo;Agentic Store\u0026rdquo; is not just automation — it’s a smart retail environment, continuously learning and adapting, helping both employees and customers thrive, while driving exceptional operational performance.\nLooking to the Future: The Future of Physical Retail The retail industry is at a pivotal moment. Physical stores will not disappear — instead, they will evolve into more powerful and efficient environments than ever before. The \u0026ldquo;Agentic Store\u0026rdquo; represents the next step in retail, combining the best aspects of physical stores with the intelligence and automation of modern cloud technology.\nSuccess in this new era requires rethinking how traditional stores operate. Retailers need to focus not just on adding new technologies but on creating an integrated, intelligent environment where systems work in harmony. The \u0026ldquo;Agentic Store\u0026rdquo; provides that framework, helping retailers transform operations while still maintaining human connection — the unique element that makes physical retail special.\nThis transformation will not happen overnight, but the roadmap is clear: retailers who embrace the \u0026ldquo;Agentic Store\u0026rdquo; model, investing both in technology and people, will be best positioned to thrive in the evolving retail landscape. Those who cling to traditional siloed methods will risk being left behind as customers increasingly demand convenience, personalization, and efficiency — all of which only \u0026ldquo;Agentic Store\u0026rdquo; can deliver.\nLearn More at the 2025 Store Operations Management Conference I’ll be sharing more insights about Agentic Stores at the Store Operations Management Conference in June 2025. If you’re interested in connecting with the store operations community and learning how AWS supports retailers in this transformation, register now to secure your spot at this meaningful event.\nAbout the Author Justin Swagler\nJustin Swagler is the Worldwide Head of Physical Retail at AWS, where he leads global strategy and thought leadership in the physical retail space. Justin has over 15 years of experience in consumer packaged goods, retail, and strategy, including innovation strategy, retail operations, product development, and senior leadership. He is passionate about guiding organizations to innovate strategically and reinvent the consumer experience. Justin holds a Bachelor’s degree from the University of Illinois, Urbana-Champaign, and an MBA from the Kellogg School of Management. "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “AI/ML/GenAI on AWS” Event Event Objectives Explore AWS AI/ML Services: Provide an overview of the AI/ML services available on AWS, helping participants understand how to apply these services in real-world projects.\nIntroduction to Generative AI: Focus on leveraging foundational models and Generative AI within AWS to build intelligent applications.\nLive Demo of Generative AI: Guide on building a chatbot using Amazon Bedrock, including techniques like prompt engineering and chain-of-thought reasoning.\nProvide Knowledge on MLOps and SageMaker: Introduce tools that manage the entire machine learning model lifecycle, from data preparation, model training to deployment and monitoring.\nKey Highlights Introduction to AI/ML on AWS: Amazon SageMaker: A comprehensive machine learning platform that helps deploy, train, and manage machine learning models.\nData Preparation and Labeling: Tools to assist with data preparation and labeling for machine learning models.\nMLOps Capabilities: Integrated features in SageMaker to support model operations (MLOps).\nLive Demo – SageMaker Studio Walkthrough: A demonstration of using SageMaker Studio to develop and deploy AI/ML models. Generative AI with Amazon Bedrock: Foundation Models: An introduction to foundational models such as Claude, Llama, Titan, and a guide to choosing the appropriate model.\nPrompt Engineering: Techniques for building effective prompts to optimize the performance of Generative AI models, including Chain-of-Thought reasoning and Few-shot learning.\nRetrieval-Augmented Generation (RAG): Architecture and integration of a knowledge base into the model generation process.\nBedrock Agents: Building multi-step workflows and integrating tools within Amazon Bedrock.\nGuardrails: Safety and content filtering measures when using Generative AI.\nLive Demo – Generative AI Chatbot with Bedrock: A demonstration of building a chatbot using Amazon Bedrock. What I Learned Design Thinking with AI/ML: Amazon SageMaker is the key platform supporting the full model lifecycle, from data preparation to model deployment and monitoring.\nPrompt Engineering: How to optimize Generative AI models using effective prompt-building techniques.\nMLOps: How to manage and automate model training, deployment, and monitoring processes in production environments.\nGenerative AI: Understanding foundational models and how they can create intelligent content, such as chatbots.\nAI/ML Architecture: Foundation Models: The differences between models like Claude, Llama, and Titan, and how to choose the right model based on project requirements.\nRAG (Retrieval-Augmented Generation): Understanding how RAG architecture works and how to integrate knowledge bases into the content creation process.\nBedrock Agents: Guidance on building complex workflows with pre-integrated tools.\nApplication to My Work Apply Amazon SageMaker: Integrate SageMaker tools into the data preparation, model training, and deployment processes for current projects.\nBuild a Generative AI Chatbot: Use Amazon Bedrock to develop chatbots and Generative AI applications for customer service or virtual assistant projects.\nOptimize AI/ML Models with Prompt Engineering: Apply prompt engineering techniques to improve AI models for better results.\nMLOps: Implement MLOps within the company to automate model training and monitoring, improving efficiency and reducing risks.\nExperience at the Event Learning from AWS Experts: The speakers shared in-depth knowledge about AI/ML and Generative AI, which helped me better understand how to use AWS tools for developing models.\nLive Demonstrations: I participated in live demos of SageMaker and Amazon Bedrock, which helped me visualize how to build and deploy AI models in real-world environments.\nNetworking and Interaction: The event allowed me to connect with other participants and experts, expanding my professional network and exchanging valuable AI/ML insights.\nConclusion The “AI/ML/GenAI on AWS” event provided me with valuable insights into the latest technologies in AI/ML, particularly around Generative AI and tools like SageMaker and Amazon Bedrock. Techniques like Prompt Engineering and MLOps have shown me the necessary steps to bring AI models from concept to production. I now feel more confident in applying these tools and techniques to real-world projects and advancing our AI/ML capabilities.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “DevOps on AWS” Event Event Objectives Understand DevOps culture and mindset: Introduce the core principles of DevOps, emphasizing collaboration, automation, and continuous improvement.\nLearn about AWS DevOps tools: Dive into AWS services that support DevOps practices, including CI/CD pipelines, Infrastructure as Code (IaC), containerization, and monitoring.\nExplore DevOps best practices: Understand key metrics like DORA and MTTR, and learn best practices for deployment, incident management, and observability.\nDemonstrate real-world use cases: Showcase AWS DevOps tools through demos and case studies, illustrating how these tools can be applied to various scenarios.\nKey Highlights Welcome \u0026amp; DevOps Mindset Recap of AI/ML session: A brief review of the previous session on AI/ML.\nDevOps culture and principles: Introduction to the cultural shift towards collaboration, automation, and continuous delivery.\nBenefits and key metrics: Overview of key DevOps metrics such as DORA (DevOps Research and Assessment), MTTR (Mean Time to Recovery), and deployment frequency.\nAWS DevOps Services – CI/CD Pipeline Source Control: Introduction to AWS CodeCommit and Git strategies like GitFlow and Trunk-based development.\nBuild \u0026amp; Test: Setting up AWS CodeBuild for automated builds and testing in CI/CD pipelines.\nDeployment: Learn about AWS CodeDeploy, with strategies for Blue/Green, Canary, and Rolling updates.\nOrchestration: AWS CodePipeline for automating the CI/CD pipeline from source to deployment.\nDemo: Full walkthrough of a complete CI/CD pipeline setup using AWS services.\nInfrastructure as Code (IaC) AWS CloudFormation: Overview of CloudFormation templates, stacks, and drift detection to manage infrastructure as code.\nAWS CDK (Cloud Development Kit): Introduction to the AWS CDK, with reusable patterns and language support.\nDemo: Deploying infrastructure using CloudFormation and CDK.\nDiscussion: Choosing between CloudFormation and CDK for IaC implementation.\nContainer Services on AWS Docker Fundamentals: An introduction to Docker, microservices, and containerization.\nAmazon ECR: Learn about Elastic Container Registry (ECR) for image storage, scanning, and lifecycle policies.\nAmazon ECS \u0026amp; EKS: Explore deployment strategies, scaling, and orchestration with Elastic Container Service (ECS) and Elastic Kubernetes Service (EKS).\nAWS App Runner: Simplified container deployment using AWS App Runner.\nDemo \u0026amp; Case Study: Comparison of microservices deployment using ECS, EKS, and App Runner.\nMonitoring \u0026amp; Observability CloudWatch: Setup and usage of AWS CloudWatch for metrics, logs, alarms, and dashboards.\nAWS X-Ray: Introduction to AWS X-Ray for distributed tracing and performance insights across microservices.\nDemo: Full-stack observability setup with CloudWatch and X-Ray for monitoring application health.\nBest Practices: Tips on alerting, dashboard setup, and creating effective on-call processes.\nDevOps Best Practices \u0026amp; Case Studies Deployment Strategies: Discuss advanced deployment strategies such as Feature flags and A/B testing for better control over application release.\nAutomated Testing and CI/CD Integration: How to integrate automated testing into your CI/CD pipelines for efficient testing and deployment.\nIncident Management and Postmortems: Best practices for handling incidents and conducting postmortem analyses to prevent future issues.\nCase Studies: Real-world examples of DevOps transformations in both startups and enterprises.\nQ\u0026amp;A \u0026amp; Wrap-up DevOps Career Pathways: Discuss various career opportunities in the DevOps field and how to develop the necessary skills.\nAWS Certification Roadmap: Overview of AWS certifications related to DevOps and how to prepare for them.\nWhat I Learned DevOps Culture and Principles: DevOps Mindset: The importance of collaboration between development and operations teams, fostering a culture of continuous improvement.\nKey Metrics: Understanding of DORA, MTTR, and deployment frequency as measures of DevOps success and efficiency.\nCI/CD Pipeline with AWS: AWS CodeCommit: Best practices for managing source code using AWS’s version control system.\nCodeBuild \u0026amp; CodeDeploy: How to configure automated build and deployment pipelines with AWS tools.\nCodePipeline Automation: The role of AWS CodePipeline in automating the entire software delivery lifecycle.\nInfrastructure as Code (IaC): AWS CloudFormation vs CDK: The benefits of using CloudFormation for managing resources declaratively, compared to the flexibility of AWS CDK for more programmatic infrastructure management. Containerization on AWS: ECR, ECS, and EKS: How to manage container images and deploy containerized applications at scale using AWS services.\nAWS App Runner: The ease of deploying applications using AWS App Runner for simpler containerization needs.\nMonitoring and Observability: CloudWatch \u0026amp; X-Ray: How to implement full-stack observability in a microservices architecture using AWS CloudWatch and X-Ray for better performance tracking and issue resolution. Application to My Work CI/CD Pipeline Setup: Implement a full CI/CD pipeline using AWS CodePipeline, CodeBuild, and CodeDeploy to automate the deployment process in my current projects.\nIaC with AWS CDK: Use AWS CDK for creating reusable infrastructure patterns and simplifying the management of cloud resources in my organization.\nContainerization with ECS \u0026amp; EKS: Deploy microservices using ECS and EKS, focusing on scaling and orchestration for better efficiency.\nImplement Observability: Set up CloudWatch and X-Ray for monitoring the health of applications in production and use insights to improve performance.\nExperience at the Event Learning from Experts: The speakers shared in-depth knowledge about DevOps principles, AWS DevOps tools, and real-world case studies, which helped me understand the implementation of DevOps practices in various environments.\nHands-on Demos: Participating in live demos and walking through the setup of CI/CD pipelines, IaC deployment, and container orchestration gave me practical insights into implementing these tools in my work.\nNetworking and Collaboration: The event allowed me to network with fellow professionals, exchange experiences, and gain insights into how other teams are implementing DevOps at scale.\nConclusion The “DevOps on AWS” event provided valuable insights into the world of DevOps and how AWS services can facilitate the implementation of modern software development practices. From automating the CI/CD pipeline to managing infrastructure as code, the event covered essential tools and strategies that can be applied directly to my work. I am excited to integrate these practices into my daily workflow to enhance automation, efficiency, and collaboration within my team.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: “AWS Well-Architected Security Pillar” Event Event Objectives Understand the Security Pillar of the AWS Well-Architected Framework: Gain insights into security best practices and how they apply to cloud architectures, focusing on AWS services and tools.\nLearn core security principles: Explore key principles such as Least Privilege, Zero Trust, and Defense in Depth to implement robust security in cloud environments.\nRecognize cloud security threats in Vietnam: Discuss common security challenges and threats faced by organizations in Vietnam’s cloud environment.\nDive deep into key AWS security pillars: Understand the five key security pillars—Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response—and how to implement them effectively.\nKey Highlights Opening \u0026amp; Security Foundation Security Pillar in Well-Architected Framework: Introduction to the importance of the Security Pillar in the AWS Well-Architected Framework and how it ensures a secure architecture.\nCore principles: Focus on Least Privilege, Zero Trust, and Defense in Depth as foundational concepts for designing secure systems.\nShared Responsibility Model: Understanding the division of responsibility between AWS and the customer for security in the cloud.\nTop threats in Vietnam\u0026rsquo;s cloud environment: Identifying the most common security threats faced by organizations operating in Vietnam\u0026rsquo;s cloud environment.\nPillar 1 — Identity \u0026amp; Access Management Modern IAM Architecture: Overview of Identity and Access Management (IAM) in AWS, including users, roles, and policies, and the importance of avoiding long-term credentials.\nIAM Identity Center: Introduction to Single Sign-On (SSO) and permission sets for managing access across AWS accounts.\nSCP \u0026amp; Permission Boundaries: How to use Service Control Policies (SCP) and permission boundaries to manage multi-account environments securely.\nMFA, Credential Rotation, Access Analyzer: Techniques for ensuring secure access through Multi-Factor Authentication (MFA), credential rotation, and access analysis.\nMini Demo: Validate IAM Policy and simulate access to see how IAM works in practice.\nPillar 2 — Detection Detection \u0026amp; Continuous Monitoring: Introduction to key services like CloudTrail, GuardDuty, and Security Hub for monitoring and detecting security events in AWS environments.\nLogging at every layer: Discuss the importance of logging at different layers (e.g., VPC Flow Logs, ALB/S3 logs) for comprehensive security monitoring.\nAlerting \u0026amp; Automation with EventBridge: Learn how to set up EventBridge for automated alerting and incident responses.\nDetection-as-Code: Implementing infrastructure and security rules using Detection-as-Code to automate the detection process.\nPillar 3 — Infrastructure Protection Network \u0026amp; Workload Security: Understanding the importance of VPC segmentation and private vs public placement in securing network traffic.\nSecurity Groups vs NACLs: Discuss the difference between Security Groups and Network Access Control Lists (NACLs) and which model is appropriate for different scenarios.\nWAF + Shield + Network Firewall: Protecting applications and networks using AWS WAF (Web Application Firewall), Shield, and Network Firewall to mitigate threats.\nWorkload Protection: Best practices for securing EC2 instances, ECS, and EKS clusters to ensure workload security.\nPillar 4 — Data Protection Encryption, Keys \u0026amp; Secrets: Introduction to AWS KMS (Key Management Service), including key policies, grants, and key rotation for secure data management.\nEncryption at Rest \u0026amp; in Transit: How to ensure encryption of data at rest (e.g., S3, EBS, RDS) and in transit (e.g., DynamoDB).\nSecrets Management: Using Secrets Manager and Parameter Store for managing sensitive data like passwords, keys, and tokens with automated rotation patterns.\nData Classification \u0026amp; Access Guardrails: Implementing data classification and access controls to ensure data protection.\nPillar 5 — Incident Response IR Playbook \u0026amp; Automation: The lifecycle of an Incident Response (IR) according to AWS, and how to automate the response process using AWS tools.\nIR Playbook: Common scenarios like a compromised IAM key, S3 public exposure, and EC2 malware detection, and how to handle them effectively.\nAuto-response with Lambda/Step Functions: Using AWS Lambda and Step Functions to automate incident response actions and mitigate damage quickly.\nQ\u0026amp;A \u0026amp; Wrap-up Summary of the 5 Pillars: A recap of the five key security pillars and their application in real-world AWS environments.\nCommon Pitfalls \u0026amp; Practices in Vietnamese Enterprises: Discussing common security challenges faced by Vietnamese companies and how to address them with AWS tools.\nSecurity Learning Roadmap: Overview of Security Specialty certification and Solutions Architect – Professional (SA Pro) certification, providing a learning path for security experts.\nWhat I Learned Security Principles \u0026amp; Pillars Core security principles: Understanding the importance of Least Privilege, Zero Trust, and Defense in Depth in the context of cloud security.\nAWS Well-Architected Security Pillars: The significance of each security pillar—Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response—in ensuring a secure cloud environment.\nCI/CD Pipeline with AWS: AWS CodeCommit: Best practices for managing source code using AWS’s version control system.\nCodeBuild \u0026amp; CodeDeploy: How to configure automated build and deployment pipelines with AWS tools.\nCodePipeline Automation: The role of AWS CodePipeline in automating the entire software delivery lifecycle.\nIAM \u0026amp; Access Control Modern IAM Architecture: Using IAM effectively to manage access securely, with tools like IAM Identity Center for SSO and MFA for strong authentication.\nMulti-account Security: How to implement SCP and permission boundaries for securing multi-account AWS environments.\nContinuous Monitoring \u0026amp; Detection CloudTrail, GuardDuty, and Security Hub: Setting up CloudTrail for activity tracking, using GuardDuty for threat detection, and aggregating security findings in Security Hub.\nLogging \u0026amp; Automation: Best practices for comprehensive logging and automated alerting with EventBridge.\nMonitoring and Observability: CloudWatch \u0026amp; X-Ray: How to implement full-stack observability in a microservices architecture using AWS CloudWatch and X-Ray for better performance tracking and issue resolution. Data Protection KMS \u0026amp; Encryption: How to implement key management and data encryption to secure sensitive data in the cloud.\nSecrets Management: Best practices for securely managing and rotating secrets with Secrets Manager and Parameter Store.\nIncident Response Automating Incident Response: Using AWS Lambda and Step Functions to automate incident response processes, improving speed and efficiency during security breaches. Application to My Work IAM Management: Implement stronger IAM policies and role-based access control (RBAC) in my organization to ensure secure and granular access management.\nContinuous Monitoring: Set up CloudTrail, GuardDuty, and Security Hub for ongoing monitoring and alerting of potential security threats.\nIncident Response Automation: Automate incident response processes using Lambda and Step Functions to reduce reaction time and improve security efficiency.\nData Encryption: Apply encryption for both data-at-rest and data-in-transit, ensuring that all sensitive data is securely handled.\nMulti-Account Security: Use SCPs and permission boundaries to manage security across multiple AWS accounts.\nExperience at the Event Learning from AWS Experts: The session provided deep insights into cloud security best practices and AWS security tools, which will help me implement effective security measures in my cloud architecture.\nHands-on Demos: I particularly enjoyed the mini demo for validating IAM policies and the incident response demo using Lambda and Step Functions, which provided practical knowledge for my own work.\nNetworking \u0026amp; Knowledge Sharing: The event allowed me to interact with security professionals, expanding my understanding of the challenges specific to cloud security in Vietnam.\nConclusion The “AWS Well-Architected Security Pillar” event was incredibly informative, providing a comprehensive understanding of cloud security best practices. The key takeaways about IAM management, continuous monitoring, data protection, and incident response automation will be directly applicable to securing our AWS environments and improving our incident response times. I now have a clearer roadmap for enhancing security practices in my organization using AWS tools.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Duy khang\nPhone Number: 0703403094\nEmail: khangndse184145@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Assurance\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Project Introduction This workshop presents the architecture and implementation of English Journey,\na vocabulary-learning web application built on AWS.\nThe application allows students to:\nsign up and sign in securely, take a level placement test (A1–C1) before learning, practise vocabulary and quizzes, track their own learning progress. On the AWS side, the project demonstrates how to combine several managed services:\nAWS Amplify as the central platform for the web app backend and hosting, Amazon Cognito for authentication, AWS Lambda for backend logic (Level test, Quiz, Vocabulary), Amazon DynamoDB for application data, Amazon SES for sending email notifications and alerts to learners (Account Verification), Amazon CloudWatch for logs, metrics, AWS WAF for basic web application protection, and IAM Roles \u0026amp; Policies to control access between all components. Workshop Objectives By the end of this workshop, a reader should be able to:\nUnderstand the overall architecture of the English Journey web app on AWS. Explain the role of Amplify and how it orchestrates Cognito, Lambda, DynamoDB and S3. Describe how the level-test feature connects frontend, Lambda and DynamoDB. Understand how notifications and system alerts are delivered via email using Amazon SES. Recognise the importance of CloudWatch and IAM for monitoring and security. Workshop Overview This project leverages AWS services to build and deploy the application:\nAWS Amplify: A full-stack hosting service that enables quick and easy deployment of applications. AWS Lambda: Handles application tasks and logic without the need to manage servers, saving costs and resources. Amazon DynamoDB: A NoSQL database used to store user data, vocabulary, and learning results. Amazon S3: Stores learning materials (videos, audio, images) to support the learning process. Amazon CloudWatch: Monitors the performance and operation of the application, providing logs and alerts in case of issues. !\n"},{"uri":"https://thienluhoan.github.io/workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":"This section summarizes the contents of the workshop you plan to conduct.\nStudying English Website 1. Executive Summary The Studying English Website is designed for learners aiming to improve their vocabulary, grammar, and daily communication skills. The platform leverages AWS Serverless services to provide learning time monitoring, predictive analysis of learners’ abilities to offer learning policies from basic to advanced levels, while minimizing costs.\n2. Problem Statement Current Problem\nEnglish is an essential language for work and daily life. However, learners currently lack space and an environment for practice, especially for communication.\nSolution\nTo address the lack of an English practice environment and support learners in improving vocabulary, grammar, and communication skills, we propose building the Studying English Website on the AWS serverless platform. It enables personalized learning based on user data, integrates listening-speaking exercises and instructional videos with learner recordings stored on S3, tracks and analyzes learning progress via AWS Lambda, ensures security and user management through Cognito, IAM, and rapidly deploys a cost-effective web interface using AWS Amplify, providing a flexible, safe, and effective learning environment while helping administrators improve teaching methods based on real data.\nBenefits and Return on Investment (ROI)\nThe Studying English Website helps learners enhance their English skills in a personalized and flexible way, reducing time and cost compared to traditional learning methods. It also provides learning progress analytics to administrators for optimized teaching methods. With low AWS infrastructure costs (~6.45 USD/month), the project has a fast ROI potential through increased learning efficiency and expanded user base, while creating valuable data for AI projects and long-term analytics.\n3. Solution Architecture The architecture of the Studying English Website is based on the AWS serverless platform, using S3 to store raw and processed data, Amplify Gen 2 for web interface deployment Route53 for DNS and routing management, Cognito for user authentication and management, Secrets Manager for sensitive information security, IAM for access control, Lambda for event-driven serverless logic, and WAF to protect the application from attacks, creating a flexible, personalized, secure, and scalable English learning system.\nAWS Services Used\nAWS Amplify Gen 2: Host the web interface AWS Route53: Manage DNS and routing AWS Cognito: Authenticate and manage users AWS IAM: Manage AWS access permissions AWS Lambda: Run serverless code triggered by events AWS WAF: Protect the web application from attacks Component Design\nData Ingestion: Data from users and sources is sent to AWS Lambda, which triggers processing workflows. Data Storage: Raw and processed data are stored in many separate S3 buckets, forming a data lake and ready-to-analyze data repository. Data Processing: AWS Lambda handles serverless events, MediaConvert converts video/audio, and data is indexed. Web Interface: AWS Amplify Gen 2 hosts a Next.js application providing dashboards, real-time analytics, and user data access. User Management: Amazon Cognito handles authentication and user access management, combined with AWS IAM for service access control, securing sensitive information via AWS Secrets Manager, and protecting the entire application with AWS WAF. DNS and routing are managed by Route53. 4. Technical Deployment Deployment Phases\nThe project consists of two parts — building the Studying English Website — each with four phases:\nResearch and Architecture Design: Study and design AWS Serverless architecture (1 month before internship). Cost Calculation and Feasibility Check: Use AWS Pricing Calculator to estimate infrastructure costs and adjust services to ensure feasibility and cost-efficiency (Month 1). Architecture Adjustment for Cost/Performance Optimization: Refine services (e.g., optimize Lambda, MediaConvert, Amplify) and data workflows for maximum efficiency (Month 2). Development, Testing, Deployment: Deploy AWS services using CDK/SDK, develop Next.js interface on Amplify, test the system, and put it into operation (Month 2–3). Technical Requirements\nThe system requires a stable internet connection to operate AWS services, including storing and retrieving data on S3, serverless data processing via Lambda, deploying Next.js web interface on Amplify Gen 2, DNS and routing management via Route53, user authentication and access control with Cognito, sensitive information security via Secrets Manager, service access control via IAM, WAF application protection, as well as supporting data analytics and real-time dashboards.\n5. Roadmap \u0026amp; Milestones Before Internship (Month 0): Study plan preparation Internship (Month 1–3): Month 1: Learn AWS and upgrade hardware Month 2: Learn deployment, plan and design architecture Month 3: Deploy, test, and launch Post Deployment: Research potential development and new features 6. Budget Estimation See costs on AWS Pricing Calculator\nOr download budget estimation file.\nInfrastructure Costs\nAWS Amplify Gen 2: 0.35 USD/month (256 MB, 500 ms request) AWS Route53: 0.50 USD/month (1 domain, 1 million queries) AWS Cognito: 0.00 USD/month (5 Free Tier users) AWS IAM: 0 USD/month AWS Lambda: 0.00 USD/month (1,000 requests, 512 MB RAM) AWS WAF: 5.00 USD/month (1 basic Web ACL) Total: 5.85 USD/month, ~70.2 USD/12 months\n7. Risk Assessment Risk Matrix\nServer downtime: High impact, medium probability Budget overrun: Medium impact, high probability Mitigation Strategy\nCosts: Use AWS Budget for alerts, optimize services Contingency Plan\nRevert to manual data collection if AWS services fail. 8. Expected Outcomes Technical Improvement: Real-time data and analytics replace manual processes. Scalable to 10–15 stations.\nLong-term Value: One-year data platform for AI research, reusable for future projects.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Research and work on the group project 11/03/2025 11/07/2025 3 - Research and work on the group project 11/03/2025 11/07/2025 4 - Research and work on the group project 11/03/2025 11/07/2025 5 - Research and work on the group project 11/03/2025 11/07/2025 6 - Research and work on the group project 11/03/2025 11/07/2025 Week 10 Achievements: Making good progress in researching and working on the group project. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Research and work on the group project 11/10/2025 11/14/2025 3 - Research and work on the group project 11/10/2025 11/14/2025 4 - Research and work on the group project 11/10/2025 11/14/2025 5 - Research and work on the group project 11/10/2025 11/14/2025 6 - Research and work on the group project 11/10/2025 11/14/2025 Week 10 Achievements: Making good progress in researching and working on the group project. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Attend AWS Cloud Mastery Series #2 event 11/17/2025 11/17/2025 3 - Continue researching and working on the group project 11/18/2025 11/21/2025 4 - Continue researching and working on the group project 11/18/2025 11/21/2025 5 - Continue researching and working on the group project 11/18/2025 11/21/2025 6 - Continue researching and working on the group project 11/18/2025 11/21/2025 Week 12 Achievements: *Making good progress in researching and working on the group project.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;CognitoPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cognito-idp:AdminCreateUser\u0026#34;, \u0026#34;cognito-idp:AdminUpdateUserAttributes\u0026#34;, \u0026#34;cognito-idp:ListUsers\u0026#34;, \u0026#34;cognito-idp:SignUp\u0026#34;, \u0026#34;cognito-idp:InitiateAuth\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;LambdaPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:ListFunctions\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:UpdateFunctionCode\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;DynamoDBPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:region:account-id:table/your-table-name\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SESPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ses:SendEmail\u0026#34;, \u0026#34;ses:SendRawEmail\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } { \u0026#34;Sid\u0026#34;: \u0026#34;CloudWatchPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudwatch:PutMetricData\u0026#34;, \u0026#34;cloudwatch:GetMetricData\u0026#34;, \u0026#34;cloudwatch:DescribeAlarms\u0026#34;, \u0026#34;cloudwatch:SetAlarmState\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;WAFPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;wafv2:CreateWebACL\u0026#34;, \u0026#34;wafv2:UpdateWebACL\u0026#34;, \u0026#34;wafv2:GetWebACL\u0026#34;, \u0026#34;wafv2:ListWebACLs\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;IAMPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:ListPolicies\u0026#34;, \u0026#34;iam:GetRole\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Technical background This workshop assumes that the reader has basic knowledge of:\nWeb development\nHTML, CSS and JavaScript/TypeScript React or another single-page application framework Cloud and AWS fundamentals\nwhat an AWS region and account are, the idea of managed services (Cognito, Lambda, DynamoDB,…), basic concepts of IAM (identity, role, policy, least privilege). The report is written so that it can be understood even if the reader does not actually deploy the system, but this background helps to follow the architecture.\nTools and services To reproduce the workshop in a real environment, the following tools and services would be required:\nAn AWS account with permission to create:\nAmplify apps, Cognito User Pools, Lambda functions, DynamoDB tables, SES identities and configuration sets, IAM roles and policies. Node.js and npm installed locally\n(for running and building the React frontend).\nThe AWS CLI configured with an IAM user or role that has sufficient permissions.\nOptionally, the Amplify CLI / Gen 2 tooling\nto define infrastructure in code and connect the project to Amplify.\nSource code and project structure The English Journey project is organised as:\na React frontend (pages such as Level Test, Dictionary, Vocabulary, My Learning), a backend defined via Amplify (Cognito, Lambda, DynamoDB), additional infrastructure for SES (email), CloudWatch and WAF. In this Hugo workshop site, we only present the architecture diagrams, explanations and example code. The actual AWS resources do not need to be created to understand the design decisions.\nThe following sections (5.3 and onwards) build on these prerequisites and explain each group of AWS services in more detail.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-create-amplify/","title":"Create Amplify backend","tags":[],"description":"","content":"Goal In this step we describe how the Amplify backend for the English Journey web application was created.\nInstead of provisioning Amazon Cognito, AWS Lambda, Amazon S3, AWS WAF and Amazon DynamoDB separately in the console, we use AWS Amplify (Gen 2) as a central orchestration layer. Amplify reads the configuration from our project and generates the necessary AWS resources for authentication, APIs, data storage and hosting.\n5.3.1 Why Amplify? English Journey is a single-page application built with React. We chose Amplify because:\nit provides one entry point to configure the backend for a web or mobile app, it can automatically create Cognito, Lambda, DynamoDB, S3 and CloudFront resources from a simple configuration, it integrates easily with the Amplify JavaScript libraries used by the frontend (sign-in, API calls, file storage). Amplify becomes the “backend platform” that hides much of the low-level boilerplate of setting up these services one by one.\n5.3.2 Amplify app creation and hosting We created a new Amplify App from the AWS Management Console and connected it to our Git repository that contains the React source code of English Journey. During the wizard we configured: the default build settings (install dependencies and run npm run build), the environment variables required by the app. Amplify automatically created: an S3 bucket to store the built static files, a CloudFront distribution in front of that bucket to serve the web site with low latency. The output of this step is a public URL where the React frontend of English Journey is hosted. All later backend resources (Cognito, Lambda, DynamoDB…) are associated with this Amplify app. 5.3.3 Authentication with Cognito (Amplify Auth) To implement sign-up, sign-in and password reset for students, we enabled the Auth category in Amplify.\nConceptually, the steps were:\nDefine the authentication requirement in Amplify: sign-in with email and password, self-registration enabled so that new students can create accounts, basic password policy and email verification. Amplify generated an Amazon Cognito User Pool with the corresponding settings. The React frontend uses the Amplify Auth library to: create new users (sign up), authenticate existing users (sign in), read user attributes (name, email) and display greetings such as “Chào mừng trở lại, Duy Khang!”. All tokens issued by Cognito (ID token and access token) are later used to protect our API and Lambda functions.\n5.3.4 Backend logic with Lambda (Amplify Functions) The main business logic of English Journey runs in AWS Lambda.\nUsing Amplify’s Function category we created several Lambda functions, for example:\nMyLearning / DailyCheckIn – updates study streaks and progress for the user. LevelTest – receives the answers from the placement test, calculates the CEFR level (A1–C2) and stores the result. Dictionary / Vocabulary – provides APIs for searching words, saving “bookmarked” vocabulary and tracking which words a user has mastered. From the Amplify project these functions are defined as backend handlers.\nWhen deploying the Amplify app, each function is created as a separate Lambda with the correct IAM permissions and environment variables (for example, DynamoDB table names).\n5.3.5 Data layer with DynamoDB To store application data we defined several data models in the backend, which Amplify maps to Amazon DynamoDB tables:\nUsers / Profiles – basic user information and learning preferences. PlacementTestResults – scores and detected level for each attempt. Vocabulary / Dictionary – list of words, meanings, examples and CEFR levels. UserProgress – saved vocabulary, words marked as “mastered”, quiz history, daily streaks. Each Lambda function receives the table name via environment variables generated by Amplify, and accesses DynamoDB through the AWS SDK.\nUsing Amplify in this way keeps all table definitions in code and makes the deployment reproducible.\n5.3.7 Protection with AWS WAF The public endpoint of the web application is the CloudFront distribution created by Amplify.\nTo protect this endpoint we associated an AWS WAF Web ACL with the distribution and enabled:\nthe AWS managed rule groups that block common web attacks (SQL injection, XSS, bot traffic), a basic rate-limit rule to prevent simple denial-of-service attempts. In the architecture diagram this is represented by the AWS WAF component in front of the Amplify application.\nSummary In summary, Amplify is the central service that creates and connects:\nCognito for authentication, Lambda for backend logic, DynamoDB for application data, and integrates with AWS WAF for additional protection. The rest of the workshop (SNS, CloudWatch, IAM policies, …) builds on top of this Amplify-managed backend.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Building a Speech Intelligence System with AssemblyAI’s Speech-to-Text Model from AWS Marketplace This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - Tapestry Makes Enterprise Knowledge More Accessible with Generative AI on AWS This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - The Agentic Store: How AI Coordination Will Revolutionize Physical Retail This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-aws-cognito/","title":"Configure AWS Cognito","tags":[],"description":"","content":"Goal In this step, we describe how Amazon Cognito is used in the English Journey web application to manage user authentication.\nStudents can sign in using:\nEmail \u0026amp; password Google (Gmail) account Cognito acts as the central identity provider that issues tokens for the frontend and backend of English Journey.\n5.4.1 Role of Amazon Cognito in the architecture In the architecture of English Journey:\nCognito User Pool stores user identities (email, name, etc.). It handles sign-up, sign-in, password reset and email verification. It integrates with Amplify so the React frontend can easily call signIn, signUp and other auth functions. It also connects to social identity providers: Google → for users who want to log in with their Gmail All successful logins (email, Google) result in Cognito tokens that are used to protect our APIs and Lambda functions.\n5.4.2 Creating the Cognito User Pool The main configuration steps:\nCreate a User Pool\nSign in to the AWS Management Console → Amazon Cognito → User pools → Create user pool. Choose Email as the primary sign-in identifier. Allow self-registration so students can create their own accounts. Configure password policy \u0026amp; verification\nSet a basic password policy (minimum length, characters, etc.). Enable email verification so students must confirm their email before using the app. Customize email templates (optional) for sign-up and password reset. Create an app client\nCreate a public app client for the web frontend. Enable Cognito User Pool and social IdPs as allowed identity providers. Configure allowed callback URLs and sign-out URLs (for example, the Amplify frontend domain of English Journey). This user pool is later referenced by Amplify and the React frontend.\n5.4.3 Enabling Google (Gmail) To support social logins, we added Google as identity providers in Cognito.\nGoogle (Gmail) login In Google Cloud Console, create an OAuth 2.0 Client ID for a web application. Set the authorized redirect URI to the Cognito callback URL generated for the user pool. Copy the Client ID and Client Secret from Google. In Cognito → User pool → Identity providers → Google: Paste the Client ID and Client Secret. Map Google attributes (email, name) to Cognito standard attributes. Now users can click \u0026ldquo;Sign in with Google\u0026rdquo; and authenticate using their Gmail account.\n5.4.4 Integrating Cognito with the Amplify frontend The React frontend of English Journey uses AWS Amplify Auth to communicate with Cognito.\nConceptually:\nFor email/password: Auth.signUp() is used to create a new user. Auth.signIn() is used for normal login. For Gmail social login: We call Auth.federatedSignIn({ provider: 'Google' }), Amplify redirects the user to Google, then back to Cognito, then back to the web app with valid tokens. On the UI, the login page shows three main options:\nSign in with email \u0026amp; password Sign in with Google All three methods still end up in the same Cognito User Pool.\n5.4.5 Security and user management With Cognito, we can:\nRequire email verification before granting full access to the application.\nRestrict which domains are allowed to use the sign-in flow (via callback URLs).\nManage users centrally:\nLock accounts Reset passwords Delete accounts Extend the solution later with:\nMFA (Multi-Factor Authentication) Additional social providers (GitHub, Facebook, …) Within the scope of this workshop, we focus on:\nSign-in with email \u0026amp; password Application login via Gmail Email verification (OTP) Summary In this step, we configure Amazon Cognito as the identity management service for English Journey:\nCreate a User Pool to manage accounts and authenticate users. Allow users to sign in with email and Google (Gmail). Use tokens issued by Cognito in the React + Amplify front end to securely access APIs and back-end services. "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: DevOps on AWS\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: ​Theo AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-create-ses/","title":"Configure Amazon SES for email","tags":[],"description":"","content":"Goal SES will be used to send transactional emails to learners\nIt can later be extended to:\nSend welcome emails when an account is created. Send study reminder emails. Note: SES is a regional service. Make sure you are working in the same AWS Region that you used for Amplify, Lambda and DynamoDB.\n5.5.1 – Open the Amazon SES console From the AWS Management Console, type “SES” in the search box. Select Amazon Simple Email Service. Check the Region in the top-right corner (for example: ap-southeast-1). If SES is in a different Region, switch to the Region used for this workshop environment. 5.5.2 – Verify an email identity For this workshop we will verify a single email address and send emails from that address (for example: your personal Gmail).\nIn the SES left navigation menu, choose Verified identities. Click Create identity. Select Email address. In Email address, enter the address you will use as the sender, for example:\nyour-name+english-journey@gmail.com. Leave the other options as default and choose Create identity. SES sends a verification email to that address. Open your inbox, find the email from Amazon Web Services, and click the verification link. Return to the SES console and refresh. The identity status should become Verified. From now on, SES will only allow sending emails from and to verified identities (SES Sandbox mode). This is enough for a classroom / workshop environment.\n5.5.3 – (Optional) Move out of Sandbox mode If later you want to send emails to real learners (unverified addresses), you must move your SES account out of sandbox mode:\nIn the SES console, go to Account dashboard. Under Your account details, check the Account status. If it is still Sandbox, click Request production access and follow the instructions. Within the scope of this workshop, you can stay in sandbox mode as long as you only send emails between verified addresses.\n5.5.4 – Create a configuration set (optional but recommended) A configuration set groups all emails from the English Journey application and enables future observability (CloudWatch metrics, event publishing, etc.).\nIn the SES navigation menu, choose Configuration sets. Click Create configuration set. Enter a name, for example: english-journey-config. Leave all other settings as default and click Create configuration set. We will use this configuration set later when sending emails from Lambda functions.\n5.5.5 – Allow Lambda to send email with SES Lambda functions such as Daily Check-in or Test Level result will send emails through SES.\nLambda needs permission to call the SES APIs.\nYou will attach this permission in Section 5.7 – Create IAM Roles \u0026amp; Policies, but we prepare the policy here for reference:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ses:SendEmail\u0026#34;, \u0026#34;ses:SendRawEmail\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/","title":"Workshop","tags":[],"description":"","content":"English Journey Overview English Journey is an innovative web application designed to help users learn English vocabulary in a structured and interactive way. This platform leverages various AWS services to provide a smooth learning experience, allowing users to track their progress, engage with dynamic content, and receive personalized feedback.\nKey Features: User Authentication: Using Amazon Cognito, users can register, log in, and securely access their learning materials.\nInteractive Learning Modules: The app offers various interactive lessons to help users expand their vocabulary.\nProgress Tracking: Users can track their progress and completion of vocabulary lessons, with detailed reports generated through AWS Lambda and DynamoDB.\nNotifications: Email notifications via Amazon SES will inform users about new lessons, progress milestones, account updates, and other important changes.\nContent Storage: All learning materials are securely stored in Amazon S3 with proper access control.\nWeb Security: To protect the platform, AWS WAF ensures the application is protected from common web threats.\nMonitoring and Alerts: AWS CloudWatch is used to monitor platform performance, with alerts configured for potential issues.\nTechnologies Used: Frontend: Built using modern web technologies, ensuring a smooth and responsive user experience.\nBackend: Powered by AWS services like Lambda and DynamoDB, ensuring scalability and performance.\nStorage: All data and media content are securely stored in Amazon S3.\nContent Workshop overview Prerequiste Create Amplify AWS Cognito Configure Amazon SES for email CloudWatch IAM Roles - Policies Configure Amazon Route 53 Clean up "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-create-cloudwatch/","title":"Configure Amazon CloudWatch","tags":[],"description":"","content":"Goal To understand how the application behaves in production and to be able to react quickly to errors, we use Amazon CloudWatch for:\ncollecting logs from Lambda functions, monitoring metrics (invocations, errors, throttles, latency), 5.6.1 CloudWatch Logs for Lambda and API By default, every Lambda function created by Amplify writes logs to CloudWatch Logs.\nIn this project, logs are used to:\nTrack requests for: Level test submissions, Quiz submissions, Dictionary / vocabulary lookups, Debug input errors, permission (IAM) errors or timeouts, Record important application events. In addition to the default logs, some functions write structured JSON logs, which makes it easier to search by userId, requestId or feature.\n5.6.2 Metrics for key services CloudWatch automatically provides metrics for:\nLambda – invocations, errors, duration, concurrent executions, DynamoDB – read/write capacity, throttled requests, S3 / CloudFront – data transfer and request counts, WAF – number of allowed/blocked requests. For the workshop we focus on a few key metrics:\nLambda Error count and Error rate for the main backend functions, Lambda Duration to detect performance issues in level test \u0026amp; quiz processing, DynamoDB ThrottledRequests to see if the provisioned capacity is sufficient. 5.6.3 CloudWatch Dashboard (optional) To quickly observe the system health, the team creates a small CloudWatch Dashboard showing:\nA chart of Lambda error rate over time, Execution duration of the LevelTest and Dictionary functions, (Optional) Number of requests blocked by AWS WAF. The dashboard is not mandatory for the workshop, but it helps illustrate the system behavior when many students are using the application.\nSummary CloudWatch completes the monitoring story for English Journey by providing:\nLogs for analysis and debugging, Metrics \u0026amp; dashboards to observe trends, Combined with Amplify, SES and WAF, this gives a reasonably robust operational setup for this workshop project.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Enhance the ability to strictly adhere to the company\u0026rsquo;s or any organization\u0026rsquo;s rules and regulations. Improve the approach and thinking process when addressing arising issues. Improve communication skills, both in daily life and at work, especially in handling situations. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.7-create-iam-roles-policies/","title":"Create IAM Roles-Policies","tags":[],"description":"","content":"Goal This section explains how IAM roles and policies are designed for the English Journey application.\nMost roles are generated by AWS Amplify, but we still need to understand:\nwhich roles exist, what each role is allowed to do (S3, DynamoDB, SES, MediaConvert, …), and how we apply the least-privilege principle. 5.7.1 Overview of IAM in the architecture In the architecture from sections 5.3–5.6, IAM is the glue that connects services:\nAmplify uses IAM roles to deploy CloudFormation stacks and host the frontend. Lambda functions use execution roles to access DynamoDB, S3 and SES (for sending emails). CloudWatch and SES rely on IAM so alarms and email notifications can be sent correctly. The design goal is that each component only receives the minimum permissions it needs.\n5.7.2 Lambda execution roles When we define backend functions in Amplify (Level Test, My Learning, Dictionary, Vocabulary, …), Amplify automatically creates a Lambda execution role for each function.\nEach role has:\nTrust policy – allows the Lambda service to assume the role:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } "},{"uri":"https://thienluhoan.github.io/workshop-template/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? I was most satisfied with the supportive environment and clear guidance from my mentor, which allowed me to learn and grow while working on tasks related to my field of study. What do you think the company should improve for future interns? More networking or team bonding events outside of working hours, and regular feedback sessions to track progress and improvements. If recommending to a friend, would you suggest they intern here? Why or why not? Yes, I would recommend it. The supportive work environment, learning opportunities, and flexible working hours are great advantages. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? More team-building events and regular feedback sessions would enhance the internship experience. Would you like to continue this program in the future? Yes, I would love to continue. The experience has been valuable, and I would like to gain more exposure. Any other comments (free sharing): The internship has been great overall. Expanding the scope of tasks for interns would make it even better. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.8-route-53/","title":"Configure Amazon Route 53","tags":[],"description":"","content":"5.8 Configure Amazon Route 53 (Custom Domain) In this step we connect the Amplify-hosted English Journey frontend to a custom domain managed by Amazon Route 53.\n🔗 Domain used in this workshop\nFor the demo environment we use the domain\nenglishjourney.xyz – the final site is available at:\nhttps://www.englishjourney.xyz/\n5.8.1 Create / verify the hosted zone Open the Route 53 console → Hosted zones → Create hosted zone. Enter your domain name, e.g. englishjourney.xyz, and keep type = Public hosted zone. Route 53 creates a set of NS and SOA records for the zone. If the domain is registered elsewhere, copy the Route 53 NS records to your registrar so that DNS is delegated to Route 53. 5.8.2 Connect the domain in AWS Amplify Go to the AWS Amplify console → select your English Journey app.\nIn the left menu choose Domain management → Add domain.\nSelect the hosted zone englishjourney.xyz.\nMap the root and sub-paths, for example:\nenglishjourney.xyz → main branch (production) www.englishjourney.xyz → redirect to root Amplify automatically creates the required A / AAAA and CNAME records in Route 53.\n5.8.3 Test the site Wait for DNS and SSL provisioning to complete (a few minutes).\nOpen a browser and navigate to:\nhttps://www.englishjourney.xyz/ Verify that the English Journey homepage is served correctly over HTTPS.\nNote this URL in your report / slides as the public entry point of the workshop application.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.9-cleanup/","title":"Clean up","tags":[],"description":"","content":"Goal This section explains how to remove the AWS resources that were created or used during the English Journey workshop, so that you do not incur unexpected costs.\nYou should only delete these resources when you have finished experimenting with the architecture.\n5.9.1 – Delete the Amplify app and front-end hosting Open the Amplify console in the same Region used for the workshop. Select the Amplify app that is hosting the English Journey front end. Choose Actions → Delete app (or the Delete button in the app details page). Confirm the deletion as instructed. When you delete the Amplify app:\nAmplify automatically removes the front-end hosting, And usually deletes the backend stacks it created (Cognito, Lambda, DynamoDB),\nunless you explicitly choose to keep them during the deletion process. Carefully read the confirmation dialog to avoid deleting important resources by mistake.\n5.9.2 – Delete any remaining back-end resources Depending on how you created the back end, there may still be some resources left after deleting the Amplify app.\nIn the AWS console, in the workshop Region, check the following services:\nCognito\nDelete any User Pools or Identity Pools that were created specifically for the workshop. Lambda\nDelete Lambda functions that only serve English Journey (for example: level test handlers, daily reminders, vocabulary processing). DynamoDB\nDelete DynamoDB tables that were used only for workshop data (learning progress, questions, vocabulary, …) if you no longer need them. 5.9.3 – Clean up SES, CloudWatch and WAF In addition to the core backend, this workshop uses Amazon SES, CloudWatch and optionally AWS WAF.\nAmazon SES Open the Amazon SES console. In Verified identities: Delete email identities that were created only for the workshop (for example: test sender or test recipient addresses). In Configuration sets: Delete the configuration set used by the English Journey application (for example: english-journey-config), if you will not reuse it. If your account was moved out of SES sandbox only for the workshop, you may want to review your SES sending quotas and usage, but there is nothing extra to delete for that.\nCloudWatch Open the CloudWatch console. In Log groups, delete: Log groups for Lambda functions that belong to English Journey. AWS WAF If you deployed a dedicated WAF Web ACL for the English Journey frontend:\nOpen the AWS WAF console. Identify the Web ACL associated with the workshop CloudFront distribution or Amplify app. If the Web ACL is used exclusively for this workshop, delete it. 5.9.4 – Clean up IAM roles and policies Finally, review IAM to ensure there are no unused roles or policies left behind:\nIn the IAM console, go to Roles:\nLook for roles created only for this workshop (for example: custom Lambda execution roles, or roles with names that clearly reference English Journey or the workshop). Before deleting a role, confirm that no Lambda function, service or user still depends on it. In Policies:\nRemove customer-managed policies that were created solely for the workshop, especially: policies that grant ses:SendEmail / ses:SendRawEmail to Lambda, policies used only by temporary roles. Do not delete shared or production IAM roles / policies that might be reused by other applications.\nAfter these steps, the AWS environment should no longer contain resources that were created specifically for the English Journey workshop.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thienluhoan.github.io/workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]